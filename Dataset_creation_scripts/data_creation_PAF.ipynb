{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义目录和文件路径\n",
    "dataset = 'PAF' # 'HARDVS', 'PAF', 'DVS128Gesture', 'SeAct'\n",
    "root_dir = '/root/autodl-tmp/{}'.format(dataset)\n",
    "base_dir = '/root/autodl-tmp/{}_Sampled_EZCLIP'.format(dataset)\n",
    "# label_mapping_file = '/root/wj/EZ_CLIP/AFE/{}/{}_idx_to_label.json'.format(dataset) # just for SeACT\n",
    "dataset_train_file = '/root/wj/EZ_CLIP/AFE/{}/{}_train.txt'.format(dataset, dataset)\n",
    "dataset_val_file = '/root/wj/EZ_CLIP/AFE/{}/{}_val.txt'.format(dataset,dataset)\n",
    "train_output_file = '/root/wj/EZ_CLIP/dataset_splits/{}/Zero-shot/train.txt'.format(dataset)\n",
    "val_output_file = '/root/wj/EZ_CLIP/dataset_splits/{}/Zero-shot/val.txt'.format(dataset)\n",
    "idx_mapping_file = '/root/wj/EZ_CLIP/AFE/{}/{}.json'.format(dataset,dataset)\n",
    "output_csv_file_labels = '/root/wj/EZ_CLIP/lists/{}_labels.csv'.format(dataset)\n",
    "# description_file = '/root/wj/EZ_CLIP/AFE/{}/{}_ds_cls.json'.format(dataset) # just for SeAct\n",
    "output_csv_file_description = '/root/wj/EZ_CLIP/GPT_discription/{}_gpt_Class_discription_new.csv'.format(dataset)\n",
    "output_file_paths = '/root/autodl-tmp/{}.txt'.format(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeAct.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 获取所有叶结点文件的绝对路径\n",
    "def get_leaf_files(dir_path):\n",
    "    leaf_files = []\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            leaf_files.append(os.path.join(root, file))\n",
    "    return leaf_files\n",
    "\n",
    "# 将文件路径写入文本文件\n",
    "def write_paths_to_file(file_paths, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for path in file_paths:\n",
    "            f.write(f\"{path}\\n\")\n",
    "\n",
    "# 获取所有叶结点文件的路径并写入文本文件\n",
    "leaf_files = get_leaf_files(root_dir)\n",
    "write_paths_to_file(leaf_files, output_file_paths)\n",
    "print(f\"文件路径已写入 {output_file_paths}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.txt & val.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/wj/EZ_CLIP/AFE/PAF/PAF_train.txt\n",
      "train.txt and val.txt files have been created at /root/wj/EZ_CLIP/dataset_splits/PAF/Zero-shot/train.txt and /root/wj/EZ_CLIP/dataset_splits/PAF/Zero-shot/val.txt\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# just for SeAct\n",
    "# 读取label映射\n",
    "with open(label_mapping_file, 'r') as f:\n",
    "    label_mapping = json.load(f)\n",
    "'''\n",
    "with open(idx_mapping_file, 'r') as f:\n",
    "    idx_mapping = json.load(f)\n",
    "\n",
    "\n",
    "# 读取train和val文件\n",
    "with open(dataset_train_file, 'r') as f:\n",
    "    print(dataset_train_file)\n",
    "    train_paths = ['/'.join((line.strip().split('/')[-2], line.strip().split('/')[-1].replace('.aedat',''))) for line in f.readlines()]\n",
    "\n",
    "with open(dataset_val_file, 'r') as f:\n",
    "    val_paths = ['/'.join((line.strip().split('/')[-2], line.strip().split('/')[-1].replace('.aedat',''))) for line in f.readlines()]\n",
    "\n",
    "# 初始化输出内容\n",
    "train_output = []\n",
    "val_output = []\n",
    "\n",
    "# 遍历目录并生成输出内容\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    if root == base_dir:  # 只遍历根目录下的子目录\n",
    "        for sub_dir in dirs: # arm crossing\n",
    "            sub_dir_path = os.path.join(base_dir, sub_dir) # /root/autodl-tmp/{}_Sampled_EZCLIP/arm crossing\n",
    "            for sub_sub_dir in os.listdir(sub_dir_path): # chenjieneng_1.1\n",
    "                full_path = os.path.join(sub_dir_path, sub_sub_dir) # /root/autodl-tmp/{}_Sampled_EZCLIP/arm crossing/chenjieneng_1.1\n",
    "                if os.path.isdir(full_path):\n",
    "                    file_list = [f for f in os.listdir(full_path) if f.endswith('.jpg')]\n",
    "                    num_files = len(file_list)\n",
    "                    label_index = idx_mapping.get(sub_dir, -1)\n",
    "                    output_line = f\"{full_path} {num_files} {label_index}\"\n",
    "                    # print(output_line)\n",
    "                    path_identifier = '/'.join(full_path.split('/')[-2:])  # 获取路径中的标识符\n",
    "                    # print(path_identifier)\n",
    "                    if path_identifier in train_paths:\n",
    "                        train_output.append(output_line)\n",
    "                    elif path_identifier in val_paths:\n",
    "                        val_output.append(output_line)\n",
    "# 写入train_output_file\n",
    "with open(train_output_file, 'w') as f:\n",
    "    for line in train_output:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "# 写入val_output_file\n",
    "with open(val_output_file, 'w') as f:\n",
    "    for line in val_output:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"train.txt and val.txt files have been created at {train_output_file} and {val_output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# labels.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been created at /root/wj/EZ_CLIP/lists/PAF_labels.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 生成label的CSV文件\n",
    "with open(idx_mapping_file, 'r') as f:\n",
    "    idx_mapping = json.load(f)\n",
    "\n",
    "# 创建一个反向映射以便快速查找类名\n",
    "idx_to_class_name = [[value,key] for key, value in idx_mapping.items()]\n",
    "\n",
    "\n",
    "# 写入CSV文件\n",
    "with open(output_csv_file_labels, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['id', 'name'])\n",
    "    for row in idx_to_class_name:\n",
    "        csvwriter.writerow(row)\n",
    "\n",
    "print(f\"CSV file has been created at {output_csv_file_labels}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# description.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been created at /root/wj/EZ_CLIP/GPT_discription/PAF_gpt_Class_discription_new.csv\n"
     ]
    }
   ],
   "source": [
    "# 生成label的CSV文件\n",
    "with open(idx_mapping_file, 'r') as f:\n",
    "    idx_mapping = json.load(f)\n",
    "ctx_prompt = 'A series of photos recording human action for '\n",
    "# 创建一个反向映射以便快速查找类名\n",
    "idx_classname_des = [[value,key,'\\n\\n' + ctx_prompt + key] for key, value in idx_mapping.items()]\n",
    "# 写入CSV文件\n",
    "with open(output_csv_file_description, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['SNo', 'Class Name', 'GPT3 discription'])\n",
    "    for row in idx_classname_des:\n",
    "        csvwriter.writerow(row)\n",
    "\n",
    "print(f\"CSV file has been created at {output_csv_file_description}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
